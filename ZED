jstest /dev/input/js0
cd /usr/local/zed/tools
./ZED_Depth_Viewer
cd
ls -la /dev/tty*
roslaunch wecar teleop.launch    //in home dir, joystick can move, machine head spin
how to move ?  >>  LB + right stick : left,right, LB + Left stick : front, back   , RB : automove
-------------------------------------------------------------자동차의 라이다 센서를 동작하고 rqt_image_view를 킴 ----------------------------------------------------------------
rosrun rviz rviz -d /home/wecar/wecar_ws/src/wecar/rviz/view_sensors.rviz  >> rviz open
rviz showdown

*How to operate rviz ?
    rviz > add laserscan > Topic > /scan
    global options > Fixed Frame > laser
    x,y,z,red sign are made
    
    add > By topic > Laserscan
rostopic echo /scan -n 1  >>>> 'laser' is 좌표계


rqt > plugin > visualization > tf Tree      >>>> 좌표계 relationship,
    odom : Km record
rqt shotdown

rivz > add > TF > all Enable uncheck > Axes uncheck > Frame Fixe : odom > odom, base_link check > and move controler : visualization move
rivz shotdown
---------------------------------------------------------------rviz 동작 방법---------------------------------------------------------------------------------------------------

rqt_graph

rostopic pub -r 5 /vesc/high_level/ackermann_cmd_mux/input/nav_0 taptap (*set speed : 1.0, steering_angle :0.1 , speed : 1.0 , angle : -0.2 is reverse): 5times in 1 secends send topic and push RB, we can see machine move 
    set only angle, speed.

new terminal onen 
rosed zed_wrapper common.yaml >> open common.yaml in vscode

* rqt > plugin > configration > Dynamic > ZED_node > video setting and we must setting vscode value(vidoe setting is not vscode setting)
don`t setting ridar

rosed zed_wrapper vesc.yaml
    vesc.driver speedmin,max 건드리지말기 ???
    steering_angle_to_servo_offset : 사용자의 요구에 맞게 변환가능 ( 일자로 된 값)

rosmsg show sensor_msgs/LaserScan : 라이다 레이져 스캔의 정보
   라이다 정면에 있는 값을 추출하려면 센터에 있는 값을 받는다.
---------------------------------------------------------------자율주행 기본 셋팅과 기본 셋팅중에 건드려야할것은 앵글과 스피드----------------------------------------------------------------------

라이다 데이터를 받아들이는 방법

cd wecar_ws/src/wecar_test/scripts  //pub와 sub을 한 파일에서 실행하는 예제
code lidar_sub.py 빈파일 생성
#! /usr/bin/env python 
    import rospy
    from sensor_msgs.msg import LaserScan
    from std_msgs.msg import Int32

    class lidar_receiver:
        def __init__(self):
            self.lidar_sub = rospy.Subscriber("/scan",LaserScan, self.callback)
            self.stop_pub = rospy.Publisher("/warning",Int32, queue_size = 5)

        def callback(self,data): #rosmsg show sensor_msgs/LaserScan에 나온 변수들을 사용가능
            print(data.anlge_min)
            for i in range(len(data.ranges)):
                angle = data.angle_min + i * data.angle_increment #각도를 구하기 라디안 기준 
                if angle > -0.01 and angle < 0.01:
                    print("{}th index range = {}".format(i,data.ranges[i]))
            if data.ranges[360] == 0.2 #m단위이므로 2cm
                print("warning")
                self.stop_pub.publish(1)
            else:
                print("safe")
                self.stop_pub.publish(0)
    def run():
        rospy.init_node("lidar_sub", anonymous=True) # 3개를 동시에 킬때 _pub 앞에 부분의 이름을 수정해야댐
        lidar_receiver_a = lidar_receiver()
        rospy.spin()
    if __name__ == "__main__":
        run()
sudo chmod +x lidar_sub.py
cd ~/wecar_ws && catkin_make
source devel/setup.bash
rospack profile
rosrun wecar_test lidar_sub.py #-3.124.. 최소측정거리보다 가깝거나 먼 값이 나오면 inf가 출력 , range는 0도가 후면기준으로 360이면 정면 180이면 오른쪽 540이면 왼쪽
-----------------------------------------------------------라이다의 각도에 해당하는 값을 출력함 -----------------------------------------------------------------------------
roscd wecar_test/scripts
cp steer_sub.py camera_sub.py
code camera_sub.py #opencv로 이미지를 띄워주는 예제
        #!  /usr/bin/env python 
        import rospy
        from sensor_msgs.msg import Image
        from std_msgs.msg import Int32
        from cv_bridge import CvBridge #opencv와 로스 이미지 사이에는 차이가 있으므로 opencv 이미지 파일형태로 변환시켜주는 모듈
        import cv2 #rostopic list 카메라의 데이터 타입을 확인 
                    rostopic info /zed2/zed_node/rgb/image_rect_color 을 치면 Image값인 것을 확인하고 코드에 Image를 import

        class lidar_receiver:
            def __init__(self):
                self.lidar_sub = rospy.Subscriber("/zed2/zed_node/rgb/image_rect_color",Image, self.callback)
                self.stop_pub = rospy.Publisher("/check",Int32, queue_size = 5)
                self.cvbridge = CvBridge()

            def callback(self,data): #rosmsg show sensor_msgs/LaserScan에 나온 변수들을 사용가능
                frame = self.cvbridge.imgmsg_to_cv2(data,"bgr8")
                cv2.imshow("frame",frame)
                cv2.waitKey(1) #이미지 창을 띄우고 기다리는것 (1ms) imshow와 세트임
                self.stop_pub.publish(1)
        def run():
            rospy.init_node("camera_sub", anonymous=True) # 3개를 동시에 킬때 _pub 앞에 부분의 이름을 수정해야댐
            lidar_receiver_a = lidar_receiver()
            rospy.spin()
        if __name__ == "__main__":
            run()
---------------------------------------------------------------카메라 프레임을 생성 ------------------------------------------------------------------------------------------

roslaunch wecar teleop만 실행된 터미널을 제외하고 모두 종료
새로운 터미널을 하나 생성
cd wecar_ws/src
catkin_create_pkg wecar_test rospy 예제를 위한 패키지 생성
cd wecar_test
mkdir scripts
cd scripts
touch steer_pub.py
sudo chmod +x steer_pub.py
vscode를 통해 steer_pub을 수정 *수정할 때 위에 두 코드의 이름을 바꿔주고 3개를 실행해야함, 이름 겹침
*rosmsg show ackerman_msgs/AckermannDriveStamped 를 하면 사용할수있는 정보의 이름을 출력해줌
        #!  /usr/bin/env python 
        import rospy
        from ackermann_msgs.msg import AckermannDriveStamped
        from std_msgs.msg import Int32
        from cv_bridge import CvBridge #opencv와 로스 이미지 사이에는 차이가 있으므로 opencv 이미지 파일형태로 변환시켜주는 모듈
        import cv2

        class lidar_receiver:
            def __init__(self):
                self.stop_sub = rospy.Subscriber("/warning",Int32, self.callback)
                self.drive_pub = rospy.Publisher("/vesc/high_level/ackermann_cmd_mux/input/nav_0",AckermannDriveStamped, queue_size = 5)

            def callback(self,data): #rosmsg show sensor_msgs/LaserScan에 나온 변수들을 사용가능
                tmp_data = AckermannDriveStamped()
                if data.data == 1:
                    tmp.data.drive.steer_angle = 0.2
                    tmp.data.drive.speed = 0.0
                else:
                    tmp.data.drive.steer_angle = -0.2
                    tmp.data.drive.speed = 1.0
                self.drive_pub.publish(tmp_data)
        def run():
            rospy.init_node("steer_pub", anonymous=True)
            lidar_receiver_a = lidar_receiver()
            rospy.spin()
        if __name__ == "__main__":
            run()

cd wecar_ws
lider_sub 실행, steer_sub 실행하면 RB를 눌렀을 때 자동으로 앞으로 가다가 라이다센서에 가까이 대면 멈춤 
----------------------------------------------------------------------------자동 멈춤 장치 코드 ---------------------------------------------------------------------------------
roscd wecar_test
mkdir launch
cd launch
code start_all.launch
        <launch>
            <node name="lidar_sub" pkg = "wecar_test" type = "lidar_sub.py" />
            <node name="camera_sub" pkg = "wecar_test" type = "camera_sub.py" /> 
            <node name="steer_pub" pkg = "wecar_test" type = "steer_sub.py" />
        </launch>
roslaunch wecar_test start_all.launch 를 하면 위에서 3개를 켰던 것과 동일하게 실행됨 
--------------------------------------------------자동 멈춤, 카메라, 라이다 센서 각도 값읽는 파일3개를 하나의 런치파일로 실행 가능------------------------------------------------
bag하는 방법
roslaunch wecar teleop.launch > 라이다 동작
cd Desktop //새로운 터미널에서
rosbag help
play랑 record를 쓸거
rosbag record /zed2/zed_node/rgb/image_rect_color /scan /vesc/low_level/ackermann_cmd_mux/input/teleop /vesc/high_level/ackermann_cmd_mux/input/nav0
/zed2/zed_node/rgb/image_rect_color >> 카메라 뷰
새로운 터미널을 열고
roscore
새로운 터미널 열고 
cd Desktop
rosbag play -l --clock bag파일//라이다는 오래된 값을 받아들이게되면 오류가남
새로운 터미널 열고
rostopic list

rosbag을 통해서 데이터를 저장하고 불러올 수 있음
----------------------------rosbag를 통해 rosbag record 노드 노드 노드 등의 데이터를 저장하기 시작 후 play를 통해 사이에 저장된 데이터 값을 읽는다---------------------------------
----------------------------------------------------------rqt_image_view 실행 후 play되는 데이터 확인----------------------------------------------------------------------------
roslaunch wecar teleop.launch
rosbag record /zed2/zed_node/rgb/image_rect_color /scan
다 끄고
roscore
rosbag play -l --clock bag 2021..파일이름
rqt_image_view 키면 play되는 데이터 확인

거리에 대한 분해능이 좋은 편임 , range값에 비례하게 거리를 조절, 
---------------------------------------------------------------rviz 화면에 직육면체를 띄우기------------------------------------------------------------------------------------
        #! /usr/bin/env python

        import rospy
        from visualization_msgs.msg import Marker

        def talker():
            pub = rospy.Publisher('test_marker',Marker,queue_size = 10)
            rospy.init_node('talker',annoymous =True)
            rate = rospy.Rate(10)
            while not rospy.is_shotdown():
                marker = Marker()
                marker.header.frame_id = "marker_frame"
                marker.type = 1
                marker.pose.orientation.w = 1
                marker.scale.x = 1.0
                marker.scale.y = 1.0
                marker.scale.z = 1.0
                marker.color.r = 1.0
                marker.color.g = 1.0
                marker.color.b = 1.0
                marker.color.a = 1.0
                pub.publish(marker)
                rate.sleep()
        if __name__ == "__main__"
            talker()
rosrun wecar_test marker_pub.py
rviz > add Marker > Fixframe = header.frame.id(marker_frame)

---------------------------------------------------------------rviz 화면에 직육면체를 띄우기------------------------------------------------------------------------------------
roslaunch zed_ar_track_alvar_example zed_ar_track_alvar.launch
rviz화면이 나옴
Fixed frame을 basic_link로 바꿔주면 뭔가뜸
left image의 iamge topic을 ../rect color로 바꿈
이후 종이를 카메라에 갖다대면 인식함

rostopic list라고 치면 visualization_marker 에 해당하는 부분을 찾을 수 있음
rostopic echo /zed/visualization_marker -n 1 을 하게되면 왼쪽 카메라를 기준으로 마커의 위치를 찾을 수 있음
google에 zed ar track 이라고 치면 맨위에 Stereolabs ZED camera 라는 사이트에서 마커를 찾을 수 있음 대회에서 0번을 쓸 예정
---------------------------------------------------------------마커를 인식하고 거리데이터와 위치데이터를 추출 -------------------------------------------------------------------
차선인식을 하는 방법
cd wecar_ws
roscd wecar_test/scripts/
cp camera_sub.py camera_test.py
code camera_test.py
        #!  /usr/bin/env python 
        import rospy
        from sensor_msgs.msg import Image
        from std_msgs.msg import Int32
        from cv_bridge import CvBridge #opencv와 로스 이미지 사이에는 차이가 있으므로 opencv 이미지 파일형태로 변환시켜주는 모듈
        import cv2 #rostopic list 카메라의 데이터 타입을 확인 
                    rostopic info /zed2/zed_node/rgb/image_rect_color 을 치면 Image값인 것을 확인하고 코드에 Image를 import

        class lidar_receiver:
            def __init__(self):
                self.lidar_sub = rospy.Subscriber("/zed2/zed_node/rgb/image_rect_color",Image, self.callback)
                self.stop_pub = rospy.Publisher("/check",Int32, queue_size = 5)
                self.cvbridge = CvBridge()

            def callback(self,data): #rosmsg show sensor_msgs/LaserScan에 나온 변수들을 사용가능
                frame = self.cvbridge.imgmsg_to_cv2(data,"bgr8")
                hsv = cv2,svtColor(frame, cv2.COLOR_BGR2HSV)
                cv2.imshow("frame",frame)
                cv2.imshow("hsv",hsv)
                cv2.waitKey(1) #이미지 창을 띄우고 기다리는것 (1ms) imshow와 세트임
                self.stop_pub.publish(1)
        def run():
            rospy.init_node("camera_sub", anonymous=True) # 3개를 동시에 킬때 _pub 앞에 부분의 이름을 수정해야댐
            lidar_receiver_a = lidar_receiver()
            rospy.spin()
        if __name__ == "__main__":
            run()
roscore
rosbag play -l --clock 21.bag
rosrun wecar_test camera_test.py
-------------------------------------------기존에 rosbag를 통해 저장된 동영상을 hsv를 통해 색을 추출해내는 예제-------------------------------------------------------------------

lane detection udacity 구글 검색해서 Ayanzadeh93의 깃허브예제를 확인
카메라를 수직에서 보는 뷰로 변환하여 차선을 인식, 대회 트랙은 흰색이 아니라 주황색으로 선이 그어져있음

이진화를 시키는 방법(camera_test 코드에 추가)
import numpy as np
callback함수에 아래 코드를 추가 *코드의 순서는 적절하게 조정해야함
    lower_th = np.array([0,0,100])
    higher_th = np.array([50,255,200])    //lower와 higher의 색상 범위를 좁히게되면 원하는 곳을 파악가능 [0,80,0],[30,255,255] 하면 노란색 색출가능
    mask =cv2.inRange(hsv,lower_th,higher_th)
    cv2.imshow("mask",mask)
    
    colored_frame = np.zeros_like(frame) //hsv로 이진화하여 흰색데이터를 얻을 곳을 and연산자를 통해 기존의 화면과 같은 색을 대입
    res = cv2.bitwise_and(colored_frame, frame, mask =mask)
    cv2.imshow("colored frame", res)
    
    pts1 = np.float32([[100,100],[200,100],[200,200],[100,200]]) //점 4개를 그린다
    pts2 = np.float32([[0,0],[640,0],[640,480],[0,480]]) //점 두개의 위치는 바꾸되 순서는 바꾸지 않는다 !
    for i in pts1:
        cv2.circle(frame, tuple(i),5,(255,0,255), -1)
    M = cv2.getPerspectiveTransform(pts1,tps2) //변환행렬을 통해 점 4개 사이의 640,480 크기의 확대한 부분을 볼 수 있다
    inv_M = cv2.getPerspectiveTransform(pts2,pts1) //변환 행렬을 계산
    BEV = cv2.warpPerspective((frame, M,(640,480))
    cv2.imshow("BEV",BEV) //가능하면 각도를 틀었을 때 차선이 모두 보이도록 설정하는 것이 좋다
    
    cv2.circle(frame, (320,240), 20,(255,0,0), -1) //첫번째 숫자는 위치, 두번째 숫자는 굵기, 세번째 숫자는 색깔, -1은 선의 굵기로 화면에 점을 표시
    
    카메라에 나오는 아랫쪽에 해당하는 부분이 실제 범퍼랑 거리가 얼마나 되는지 먼저 확인해야한다.
    차선의 기울기를 인식하면 좀 더 부드럽게 갈 수 있음
    바퀴각도 계산
    steering_angle = distance * kd + angle * ka 차선의 기울기와 차선과의 거리를 적절한 상황에 계산하여 사용하면 더 좋은 결과를 도출 할 수있음
    
    0. 차선인식을 해라
    1. 완주를 할 수 있는 차량을 만들어라
    2. 나머지 미션들을 준비하면 됨
    
    wecar에서 lane_detection , obstacle_detector(클러스터링, 트래킹해주는 패키지), 신호등을 이미지 처리로하는 것이 아니고 특정한 토픽으로 들오게끔해줄거임,
    그 토픽받아서 동작하는 것만 수행하면됨(신호등의 색깔 걱정안해도됨 )
    -----------------------------라이다를 통해 점 가운데를 기준으로 +30도에 range가 0.5 이내인 점 10개 이상이면 steer가 왼쪽 -30~0도에 찍히면 오른쪽으로 틀고, 둘다 10개 이상이면
    정지, 둘다 없으면 1.0m/s로 직진
    
    
    
    


